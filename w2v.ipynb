{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n2QqmjC0esQ"
      },
      "source": [
        "# Word2Vec Application Tutorial\n",
        "In this tutorial, we go over basic operations on word vectors. There are many Natural Language Processing (NLP) libraries in Python, such as [NLTK](https://www.nltk.org/), [gensim](https://radimrehurek.com/gensim/), and [spaCy](https://spacy.io/). All of them have their own strength and focus. NLTK is one of the first comprehensive Python libraries for computational linguistics and has a big community. If you have worked on NLP, you probably have heard of it or used it. Gensim is a popular library for topic modeling. It also provides many functionalities similar to NLTK. It supports word embeddings and you can even train word embeddings using gensim. SpaCy is another popular NLP library and it provides built-in support for word vectors. We will use spaCy in this tutorial.  \\\\\n",
        "<br>\n",
        "You will learn:\n",
        "\n",
        "\n",
        "1.   Popular Python machine learning packages (spaCy, sklearn)\n",
        "2.   Calculating word similarity using Word2Vec model\n",
        "3.   Word analogy analysis\n",
        "4.   Calculating sentence similarity using Word2Vec model\n",
        "5.   Dimension reduction techniques for high-dimensional vectors\n",
        "6.   Visualizing Word2Vec in 2D space\n",
        "7.   Sentiment analysis using logistic regression and Word2Vec\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fymQg_Q068-"
      },
      "source": [
        "### Preliminary\n",
        "First, let's install the spaCy Python library and download their model for the English language. We only need to do it once. Then we can import the spaCy library and other useful libraries such as numpy (used for linear algebra and vector operations in Python). We can load our downloaded English model in our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VJCepyC9ZbX",
        "outputId": "4e3871c5-4382-4393-dd09-914cba82ad75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "2022-11-04 22:09:41.978146: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 587.7 MB 15 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.4.1) (3.4.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "# Only needs to be run once at the top of the notebook\n",
        "!pip3 install spacy\n",
        "!python3 -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzPVan-OAIw8"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import spacy\n",
        "import numpy as np\n",
        "import csv\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import linear_model\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9wiQz4BAuQT"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_lg')  # load the English model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfpycYEG2T3y"
      },
      "source": [
        "### Word Similarity\n",
        "By representing words in vectors, we can use linear algebra and vector space models to analyze the relationship between words. One simple task is to calculate the cosine of two word vectors, namely the cosine similarity. This cosine similarity measures the semantic similarity of words. While the value ranges from -1 to 1, it is usually used in the non-negative space [0, 1] where 0 means 0 similarity and 1 means extremely similar or even identical. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm-5ZI0gHp-w"
      },
      "source": [
        "In order to calculate the cosine similarity between words, we have to know their vector representations first, which are provided by the Word2Vec model. In the spaCy English model, these vector representations (pretrained using Word2Vec) are already provided. All we need to do is to retrieve these words from the spaCy English model and we will have access to these vector representations. \\\\\n",
        "<br>\n",
        "![cosine_sim](https://engineering.aweber.com/wp-content/uploads/2013/02/4AUbj.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-HL5bHMA3RE",
        "outputId": "abc26f50-35fb-45e6-af2e-be31e925db4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector length: 300\n",
            "cat: [ 3.7032e+00  4.1982e+00 -5.0002e+00 -1.1322e+01  3.1702e-02 -1.0255e+00\n",
            " -3.0870e+00 -3.7327e+00  5.3875e-01  3.5679e+00  6.9276e+00  1.5793e+00\n",
            "  5.1188e-01  3.1868e+00  6.1534e+00 -4.8941e+00 -2.9959e-01 -3.6276e+00\n",
            "  2.3825e+00 -1.4402e+00 -4.7577e+00  4.3607e+00 -4.9814e+00 -3.6672e+00\n",
            " -1.8052e+00 -2.1888e+00 -4.2875e+00  5.5712e+00 -5.2875e+00 -1.8346e+00\n",
            " -2.2015e+00 -7.7091e-01 -4.8260e+00  1.2464e+00 -1.7945e+00 -8.1280e+00\n",
            "  1.9994e+00  1.1413e+00  3.8032e+00 -2.8783e+00 -4.2136e-01 -4.4177e+00\n",
            "  7.7456e+00  4.9535e+00  1.7402e+00  1.8275e-01  2.4218e+00 -3.1496e+00\n",
            " -3.8057e-02 -2.9818e+00  8.3396e-01  1.1531e+01  3.5684e+00  2.5970e+00\n",
            " -2.8438e+00  3.2755e+00  4.5674e+00  3.2219e+00  3.4206e+00  1.1200e-01\n",
            "  1.0303e-01 -5.8396e+00  4.6370e-01  2.7750e+00 -5.3713e+00 -5.0247e+00\n",
            " -2.0212e+00  5.8772e-01  1.1569e+00  1.3224e+00  4.3994e+00  2.0444e+00\n",
            "  2.1343e+00 -1.9023e+00  2.1469e+00 -2.9085e+00  4.8429e-01 -3.3544e-01\n",
            "  1.4484e+00 -1.5770e+00 -1.1307e+00  2.8320e+00  6.2041e-01  3.7994e+00\n",
            " -3.1162e-01 -6.9221e+00  7.1342e+00  7.2441e+00 -8.9326e+00 -2.7927e+00\n",
            "  2.6613e-01  6.7547e-01  6.7293e+00 -5.8127e+00  3.1567e+00 -1.0634e+00\n",
            " -1.5733e+00  1.3534e+00  3.9218e-01 -8.7077e+00  3.4229e-02  3.3251e+00\n",
            "  4.6713e+00  1.1865e-02  9.8345e-01 -5.3206e-02 -9.1613e+00  6.0161e+00\n",
            " -2.2223e+00  2.5015e+00 -6.0702e-01 -3.6344e-02  7.1884e+00 -1.4431e+00\n",
            "  2.6156e+00 -1.0148e+00  4.1225e+00 -1.8472e+00  4.6292e+00 -2.6506e+00\n",
            " -1.8937e+00  4.1749e+00 -9.6644e+00 -2.4813e+00 -2.7637e+00 -1.0624e+00\n",
            "  3.5988e+00  4.9833e+00  6.4499e-01  2.5784e-01  9.8727e-01 -4.2485e+00\n",
            "  3.4272e-01 -2.2270e+00 -1.8957e+00  8.0796e-01 -2.0265e+00 -6.1828e+00\n",
            " -2.2378e+00  2.8216e+00 -2.0050e+00 -3.8924e+00 -2.9364e-01 -1.6128e+00\n",
            " -6.7874e-01 -1.9855e+00  1.8221e-01  2.1575e+00  4.9825e-01 -1.7326e+00\n",
            "  4.7886e+00  2.9904e+00  8.3447e-01 -4.7417e+00  2.4697e+00  1.3751e+00\n",
            "  4.5358e+00  6.5386e-01  5.5413e+00  2.3963e+00  1.0031e+00 -8.0664e-01\n",
            " -1.4126e+00  2.8689e+00 -8.7339e+00 -2.7457e+00 -3.1805e-01 -2.4484e-01\n",
            "  3.7117e+00 -1.8636e+00  2.9959e-01  6.5062e-02 -1.5682e+00  1.5876e+00\n",
            "  6.9224e-01 -6.7734e+00  3.1065e+00  2.3973e+00 -3.5138e+00  3.4460e+00\n",
            "  3.4252e+00 -5.1906e+00 -6.9372e-01  1.9435e+00 -1.5669e-01  1.9710e+00\n",
            "  8.7743e-01 -8.3110e+00 -4.0306e-01 -5.0165e+00 -5.6309e-02  4.9249e+00\n",
            " -7.1053e+00 -5.2338e+00  2.3535e+00 -2.5255e+00 -2.7785e+00  5.0149e+00\n",
            " -2.8405e+00 -1.8614e+00  2.8818e-03  1.3281e+00  1.0194e+00  3.5155e+00\n",
            "  2.7971e-01  1.3251e+00  1.4386e+00 -6.1719e-01 -2.6864e+00 -3.9613e+00\n",
            "  4.5749e+00 -1.0939e+00  1.3289e+00 -9.5484e-01 -5.4675e+00  2.1607e+00\n",
            "  5.0715e-01  1.4860e-01 -4.8571e+00 -2.2213e+00 -2.3498e-01 -4.2629e+00\n",
            " -8.7002e-01  3.3796e+00 -4.3989e+00  6.1047e+00  3.7927e+00 -6.0760e+00\n",
            "  3.1840e+00 -8.3104e-01 -5.4015e+00 -6.2916e+00  1.2497e+00  1.8026e+00\n",
            " -3.4535e+00 -2.1652e-01 -1.4958e+00  5.7946e-01  2.2505e+00  2.0868e+00\n",
            "  3.9621e-01  1.6076e+00  4.0635e+00 -3.4088e+00 -1.0590e+00 -3.6376e+00\n",
            "  2.0501e+00  1.4785e+00 -1.8906e+00 -2.6215e-01 -5.1386e+00  3.7029e+00\n",
            " -1.8151e+00 -3.2759e+00 -5.1866e+00  2.5485e-01 -4.5696e+00  1.0147e+01\n",
            " -3.0195e+00 -2.4640e+00  7.5459e-01 -5.6395e+00 -5.4095e+00 -2.4363e+00\n",
            " -4.3922e-01 -4.0911e+00 -3.5194e+00  1.8031e+00 -1.3644e-01  6.7990e+00\n",
            "  5.8461e+00  5.3452e-01  1.1042e+00  3.5698e+00  4.4668e+00 -2.4537e+00\n",
            " -2.1832e+00  1.5293e+00 -1.9414e+00 -8.8675e-02 -1.1825e+00 -3.9996e+00\n",
            "  2.8077e+00 -1.8000e+00  4.2545e+00 -1.3813e+00 -2.2921e+00  3.7889e+00\n",
            " -1.5837e+00 -7.2078e-01  4.7743e+00 -3.0923e+00  8.4709e+00  3.0132e-01\n",
            " -5.6173e+00 -5.4610e-01 -4.8459e+00  6.0303e+00 -6.9664e+00  3.1445e+00]\n"
          ]
        }
      ],
      "source": [
        "# retrieve words from the English model vocabulary\n",
        "cat = nlp.vocab['cat']\n",
        "dog = nlp.vocab['dog']\n",
        "car = nlp.vocab['car']\n",
        "\n",
        "# print the dimension of word vectors\n",
        "print('vector length:', len(cat.vector))\n",
        "\n",
        "# print the word vector\n",
        "print('cat:', cat.vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmpDwypeHSIy"
      },
      "source": [
        "Try to retrieve some other words and check if they have the same dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO5S4yX9hgZg",
        "outputId": "3500be8f-1137-4cbf-9083-91e270aa6db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ],
      "source": [
        "# try your own words and check if they have the same dimension of the cat vector\n",
        "############# YOUR CODE HERE ################\n",
        "computer = nlp.vocab['computer']\n",
        "\n",
        "print(len(computer.vector))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g5mO0AfIT94"
      },
      "source": [
        "After retrieving the words and their vector representations, we can use the built-in similarity function (which implements cosine similarity) to calculate word similarity based on these vectors. Is 'cat' more similar to 'dog' than 'car'? Can you find some properties of cosine similarity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpxnslv1DB7e",
        "outputId": "5f250bb8-a3c4-4c91-b982-5f34c01c46b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity between cat and cat: 1.0\n",
            "The similarity between cat and dog: 0.8220816850662231\n",
            "The similarity between dog and cat: 0.8220816850662231\n",
            "The similarity between cat and car: 0.19698593020439148\n",
            "The similarity between dog and car: 0.3250025510787964\n"
          ]
        }
      ],
      "source": [
        "# you can calculate the similarity between words using \n",
        "# the built-in 'similarity' function\n",
        "print('The similarity between cat and cat:', cat.similarity(cat))\n",
        "print('The similarity between cat and dog:', cat.similarity(dog))\n",
        "print('The similarity between dog and cat:', dog.similarity(cat))\n",
        "print('The similarity between cat and car:', cat.similarity(car))\n",
        "print('The similarity between dog and car:', dog.similarity(car))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe1G8CvuIxIP"
      },
      "source": [
        "Now let's try some other words. Also, try to calculate the cosine similarity between 'hotel' and 'motel' and the cosine similarity between 'hotel' and 'hospital'. Which one is more similar to 'hotel'? 'motel' or 'hospital'?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QALZbd0Jhjlf",
        "outputId": "f288a2e5-7adb-448b-f0f2-94d8dfc5c307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7972080111503601\n",
            "0.4250935912132263\n"
          ]
        }
      ],
      "source": [
        "# calculate the similarity of your own words using the built-in function\n",
        "############# YOUR CODE HERE ################\n",
        "hotel = nlp.vocab['hotel']\n",
        "motel = nlp.vocab['motel']\n",
        "hospital = nlp.vocab['hospital']\n",
        "\n",
        "# what is the similarity between (hotel, motel) and (hotel, hospital)\n",
        "############# YOUR CODE HERE ################\n",
        "print(hotel.similarity(motel))\n",
        "\n",
        "print(hotel.similarity(hospital))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jwt8kG5Kuof"
      },
      "source": [
        "Let's compute the cosine similarity manually using its definition below. Then check if the result is the same as the one calculated by the built-in function. \\\\\n",
        "<br>\n",
        "$cosine\\_similarity(A, B) = \\frac{A \\cdot B}{\\left \\| A \\right \\|\\left \\| B \\right \\|}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3ZzlyeLhm3e",
        "outputId": "b4fded13-c036-47ae-a3d7-aa16f6f58696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity between dog and car calculated manually: 0.32500255\n"
          ]
        }
      ],
      "source": [
        "# try to calculate cosine similarity manually\n",
        "'''\n",
        "cosine of V1 and V2 = dot product of V1 and V2 / product of V1 norm and V2 norm\n",
        "To get the vector representation of a word, use .vector, e.g. car.vector\n",
        "To calculate the dot product of two vectors V1 and V2, use np.dot(V1, V2)\n",
        "To get the norm of a word vector, use .vector_norm, e.g. car.vector_norm, \n",
        "alternatively you can use np.linalg.norm(V1) to calculate the norm of V1\n",
        "'''\n",
        "############# YOUR CODE HERE ################\n",
        "cosine_dog_car = np.dot(dog.vector, car.vector)/(dog.vector_norm*car.vector_norm)\n",
        "print('The similarity between dog and car calculated manually:', cosine_dog_car)\n",
        "#############################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma4PgQeaMZfM"
      },
      "source": [
        "Now we know how to compare the similarity of two words using pretrained Word2Vec model. We can also use it to help us find semantically similar words, that is given a word retrieve similar words from the vocabulary. \\\\\n",
        "<br>\n",
        "The Python spaCy library hasn't provided such a function to do precisely this yet. We could use other NLP and machine learning libraries, such as [gensim](https://radimrehurek.com/gensim/), to do this with a simple function call. But the implementation is not hard, so let's give it a try! In our customized function, we first find all the words in our vocabulary (that has vector representations). Then we calculate the cosine similarity between our query word and each word in the vocabulary. We sort the similarity score in descending order. Finally, we retrieve the top n most similar words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6dRHeE6KBq5"
      },
      "outputs": [],
      "source": [
        "# function to find similar words\n",
        "def most_similar(word, topn=10):\n",
        "    allwords = [w for w in nlp.vocab if w.has_vector and w.is_lower and w.lower_ != word.lower_]  # get all words in the vocabulary\n",
        "    by_similarity = sorted(allwords, key=lambda w: word.similarity(w), reverse=True)  # sort words by similarity in descending order\n",
        "    return by_similarity[:topn]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCemJQbROBpi",
        "outputId": "99387819-1cb0-4b69-8cf6-e0243279a160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar words to cat:  ['cat', 'you', 'car', 'somethin’', 'lovin’', \"'cause\", 'lovin', 'she', \"somethin'\", 'it']\n"
          ]
        }
      ],
      "source": [
        "# find similar words\n",
        "cat_similar = [w.text for w in most_similar(dog)]\n",
        "print('Similar words to cat: ', cat_similar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYclgBII2gc7"
      },
      "source": [
        "### Word Analogy\n",
        "One interesting finding for the Word2Vec model is that it embeds some analogical relationships between words. \\\\\n",
        "<br>\n",
        "*Man is to Woman as King is to Queen* \\\\\n",
        "Man - Woman = King - Queen \\\\\n",
        "<br>\n",
        "*Paris is to France as Madrid is to Spain* \\\\\n",
        "Paris - France = Madrid - Spain \\\\\n",
        "<br>\n",
        "These relationships can be reconstructed using word embeddings. \\\\\n",
        "<br>\n",
        "![analogy](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/06062705/Word-Vectors.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6Xo9Prcfpoy"
      },
      "outputs": [],
      "source": [
        "# word analogy example\n",
        "# king is to man as what is to woman?\n",
        "king = nlp.vocab['king']\n",
        "man = nlp.vocab['man']\n",
        "woman = nlp.vocab['woman']\n",
        "\n",
        "# resulting vector\n",
        "result = king.vector - man.vector + woman.vector\n",
        "\n",
        "# function to compute cosine similarity\n",
        "cosine = lambda v1, v2: np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg_R5y2MyWFv",
        "outputId": "b95d1d5b-51be-487e-f930-a194d9b103a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between queen and result: 0.6178014\n",
            "['queen', 'and', 'that', 'r.', 'where']\n"
          ]
        }
      ],
      "source": [
        "# what word does the 'result' vector closely correspond to?\n",
        "\n",
        "# we can first check if the 'result' vector is similar to the 'queen' vector\n",
        "############# YOUR CODE HERE ################\n",
        "queen = nlp.vocab['queen']\n",
        "print('Similarity between queen and result:', cosine(result, queen.vector))\n",
        "#############################################\n",
        "\n",
        "# find all words in our vocabulary (nlp.vocab), \n",
        "# make sure to just retrieve lower case words \n",
        "# and words that actually have vectors (.has_vector) \n",
        "# and filter out 'king', 'man', and 'woman'\n",
        "############# YOUR CODE HERE ################\n",
        "allwords = [w for w in nlp.vocab if w.has_vector and w.is_lower and w.lower_ != 'king' and w.lower_ != 'man' and w.lower_ != 'woman']\n",
        "#############################################\n",
        "\n",
        "# calculate the cosine similarity between the 'result' vector \n",
        "# and all word vectors in our vocabulary\n",
        "# sort by similarity and print out the most similar one\n",
        "############# YOUR CODE HERE ################\n",
        "candidates = sorted(allwords, key=lambda w: cosine(result, w.vector), reverse=True)\n",
        "print([c.text for c in candidates[:5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHDhFaO2VaFJ"
      },
      "source": [
        "Let's try: \\\\\n",
        "Paris - France = Madrid - Spain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unaXunKv0Y0F",
        "outputId": "8859ca2b-9225-4b49-97a8-fc8f01b96314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and', 'king', 'Colo', 'Calif', 'were']\n"
          ]
        }
      ],
      "source": [
        "# another example\n",
        "# Paris is to France as Madrid is to what?\n",
        "############# YOUR CODE HERE ################\n",
        "Paris = nlp.vocab['Paris']\n",
        "France = nlp.vocab['France']\n",
        "Madrid = nlp.vocab['Madrid']\n",
        "\n",
        "maybe_Spain = France.vector - Paris.vector + Madrid.vector\n",
        "\n",
        "allwords = [w for w in nlp.vocab if w.has_vector and w.lower_ != 'paris' and w.lower_ != 'madrid' and w.lower_ != 'france']\n",
        "candidates = sorted(allwords, key=lambda w: cosine(maybe_Spain, w.vector), reverse=True)\n",
        "print([c.text for c in candidates[:5]])\n",
        "#############################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFPYDPuX6thm"
      },
      "source": [
        "### Sentence/Document Level Similarity\n",
        "Using word embeddings, we can also calculate similarity between sentences and documents. More advanced models such as Doc2Vec or neural networks can be used, but in this tutorial we will continue to use Word2Vec model to calculate document similarity. Since sentences and documents are composed of words, one easy way to obtain vector representations for sentences/documents is to calculate the average vectors of words. \\\\\n",
        "<br>\n",
        "Let's try to calculate the similarity among these three sentences:\n",
        "\n",
        "\n",
        "1.   Cats are beautiful animals.\n",
        "2.   Some gorgeous creatures are felines.\n",
        "3.   Dolphins are swimming mammals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiS9WQyG8zqG",
        "outputId": "6f52c9e7-5400-4f25-8bf9-748491af6882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.90675163\n",
            "0.9037428\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec model does not provide vector representations for sentences \n",
        "# or documents. How is the similarity between sentences computed?\n",
        "# Since sentences are composed of words, an easy way to obtain the vector\n",
        "# representations of sentences is by averaging the vectors of each word in\n",
        "# the sentence.\n",
        "############# YOUR CODE HERE ################\n",
        "s1 = (nlp.vocab['Cats'].vector + nlp.vocab['are'].vector + nlp.vocab['beautiful'].vector + \\\n",
        "    nlp.vocab['animals'].vector + nlp.vocab['.'].vector)/5\n",
        "s2 = (nlp.vocab['Some'].vector + nlp.vocab['gorgeous'].vector + nlp.vocab['creatures'].vector + \\\n",
        "    nlp.vocab['are'].vector + nlp.vocab['felines'].vector + nlp.vocab['.'].vector)/6\n",
        "s3 = (nlp.vocab['Dolphins'].vector + nlp.vocab['are'].vector + nlp.vocab['swimming'].vector + \\\n",
        "    nlp.vocab['mammals'].vector + nlp.vocab['.'].vector)/5\n",
        "\n",
        "print(cosine(s1, s2))\n",
        "\n",
        "print(cosine(s1, s3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_P6Ep9F67Lu",
        "outputId": "76798b73-7b91-4be3-93c8-8f1bdbf99c71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between target and doc1: 0.9067517259890845\n",
            "Similarity between target and doc1: 0.9037427153904276\n"
          ]
        }
      ],
      "source": [
        "# spaCy also supports similarity calculation between sentences and documents\n",
        "target = nlp(\"Cats are beautiful animals.\")  # text about cats\n",
        "\n",
        "doc1 = nlp(\"Some gorgeous creatures are felines.\")  # text about cats\n",
        "doc2 = nlp(\"Dolphins are swimming mammals.\")  # text about dolphins\n",
        "\n",
        "print('Similarity between target and doc1:', target.similarity(doc1))\n",
        "print('Similarity between target and doc1:', target.similarity(doc2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYs9_tHSFBhd"
      },
      "source": [
        "### Word Embeddings Visualization\n",
        "Since the word vectors we use have 300 dimensions, we cannot visualize them. One natural way is to apply dimension reduction first and then visualize them. We use a popular dimension reduction technique called [t-SNE](https://lvdmaaten.github.io/tsne/) (you can also use PCA) to reduce the word vectors to 2D and then plot the words in our word analogy example to see if we can find some pattern visually. \\\\\n",
        "<br>\n",
        "An interactive visualization of word embeddings can be found here: \\\\\n",
        "[https://projector.tensorflow.org/](https://projector.tensorflow.org/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "HUnAim1BFH50",
        "outputId": "0fd6a2db-3ca6-45aa-f375-25c12544a1bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# use t-SNE to do dimension reduction, from 300d to 2d\n",
        "tsne_model = TSNE(n_components=2)\n",
        "\n",
        "# get transformed vectors\n",
        "data = np.array([king.vector, man.vector, queen.vector, woman.vector])\n",
        "data_2d = tsne_model.fit_transform(data)\n",
        "\n",
        "labels = ['king', 'man', 'queen', 'woman']\n",
        "\n",
        "# plot the 2d vectors and show their labels\n",
        "plt.scatter(data_2d[:, 0], data_2d[:, 1], s=100)\n",
        "for i, txt in enumerate(labels):\n",
        "    plt.annotate(txt, (data_2d[i,0], data_2d[i,1]), xytext=(2, 3), textcoords='offset points')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXzQ0M-OW0h0"
      },
      "source": [
        "### Sentiment Analysis\n",
        "The major reason for coming up with word embedding models is that we want to use these embeddings which encode the word semantics to help us tackle problems related with natural language. \\\\\n",
        "<br>\n",
        "One such task is sentiment analysis. By analyzing the sentiment of texts, we want to understand whether a given sentence/document is positive or negative. For example, 'the weather is so nice today' has a positive sentiment whereas 'he is bored by the movie' has a negative sentiment. \\\\\n",
        "<br>\n",
        "In this tutorial, we want to use the word embeddings combined with a simple machine learning model ([logistic regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)) to do sentiment analysis. Logistic regression is a linear classification model and in our case we want to classify whether a given sentence is positive or negative. So it's a binary classification. \\\\\n",
        "<br>\n",
        "![logistic](https://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression_files/logistic_regression_schematic.png)\n",
        "<br>\n",
        "Our training data contains 2,748 from Yelp reviews, IMDB movie reviews, and Amazon reviews. In the dataset, 1 means positive and 0 means negative. The original data can be downloaded from [here](https://www.kaggle.com/rahulin05/sentiment-labelled-sentences-data-set/data), the combined file can be downloaded from [here](https://drive.google.com/file/d/1knrjvDNkiXtviXBoLm5OJY45_kcCmDxe/view?usp=sharing)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}